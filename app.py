# -*- coding: utf-8 -*-
"""app.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Bj5XnKXsGDtWBBt1op37gu9D9sWuE_8O
"""

import streamlit as st
import pickle
import json
import random
from sklearn.metrics.pairwise import cosine_similarity

# Set page title and icon
st.set_page_config(
    page_title="Medical Textbook Chatbot",
    page_icon="ðŸ©º",
    layout="centered"
)

# Custom CSS to make it look like a chat interface
st.markdown("""
<style>
.user-bubble {
    background-color: #e1f5fe;
    padding: 10px 15px;
    border-radius: 20px 20px 0 20px;
    margin: 10px 0;
    max-width: 80%;
    margin-left: auto;
    margin-right: 10px;
}
.bot-bubble {
    background-color: #f1f1f1;
    padding: 10px 15px;
    border-radius: 20px 20px 20px 0;
    margin: 10px 0;
    max-width: 80%;
    margin-right: auto;
    margin-left: 10px;
}
.chat-container {
    height: 400px;
    overflow-y: auto;
    display: flex;
    flex-direction: column;
    padding: 10px;
    border: 1px solid #ddd;
    border-radius: 10px;
}
</style>
""", unsafe_allow_html=True)

# Load data files
@st.cache_resource
def load_data():
    try:
        with open('chunks.json', 'r') as f:
            chunks = json.load(f)

        with open('chunk_sources.json', 'r') as f:
            chunk_sources = json.load(f)

        with open('vectorizer.pkl', 'rb') as f:
            vectorizer = pickle.load(f)

        with open('chunk_vectors.pkl', 'rb') as f:
            chunk_vectors = pickle.load(f)

        return chunks, chunk_sources, vectorizer, chunk_vectors
    except Exception as e:
        st.error(f"Error loading data: {e}")
        return None, None, None, None

# Search function
def search(query, chunks, chunk_sources, vectorizer, chunk_vectors, top_k=3):
    query_vector = vectorizer.transform([query])
    similarities = cosine_similarity(query_vector, chunk_vectors).flatten()
    top_indices = similarities.argsort()[-top_k:][::-1]

    results = []
    for idx in top_indices:
        if similarities[idx] > 0.1:  # Only return somewhat relevant results
            results.append({
                'chunk': chunks[idx],
                'source': chunk_sources[idx],
                'score': similarities[idx]
            })

    return results

# Function to generate a conversational response
def generate_response(query, results):
    if not results:
        responses = [
            "I couldn't find specific information about that in your medical textbooks. Could you rephrase your question?",
            "I don't have enough information about that topic in my database. Can you ask something else?",
            "That's not covered in the medical textbooks I have access to. Is there another medical topic you're curious about?"
        ]
        return random.choice(responses)

    # Sort by relevance score
    best_result = max(results, key=lambda x: x['score'])

    # Extract the most relevant chunk
    content = best_result['chunk']
    source = best_result['source']

    # Generate different response templates
    templates = [
        f"According to the textbook {source['pdf']} (page {source['page']}): {content}",
        f"I found this information that might help: {content}\n\nThis comes from {source['pdf']}, page {source['page']}.",
        f"Here's what I know about that: {content}\n\nSource: {source['pdf']}, page {source['page']}."
    ]

    return random.choice(templates)

# Initialize session state
if 'messages' not in st.session_state:
    st.session_state.messages = [
        {"role": "assistant", "content": "Hello! I'm your medical textbook assistant. What would you like to know about?"}
    ]

# Main app
def main():
    st.title("Medical Textbook Chatbot ðŸ©º")

    chunks, chunk_sources, vectorizer, chunk_vectors = load_data()

    if not chunks:
        st.error("Failed to load medical textbook data. Please check your setup.")
        return

    # Display chat messages
    chat_container = st.container()
    with chat_container:
        st.markdown('<div class="chat-container">', unsafe_allow_html=True)
        for message in st.session_state.messages:
            if message["role"] == "user":
                st.markdown(f'<div class="user-bubble">{message["content"]}</div>', unsafe_allow_html=True)
            else:
                st.markdown(f'<div class="bot-bubble">{message["content"]}</div>', unsafe_allow_html=True)
        st.markdown('</div>', unsafe_allow_html=True)

    # Chat input
    with st.form("chat_input", clear_on_submit=True):
        user_input = st.text_input("Type your medical question:", key="user_query")
        submit_button = st.form_submit_button("Send")

        if submit_button and user_input:
            # Add user message to chat
            st.session_state.messages.append({"role": "user", "content": user_input})

            # Get response
            results = search(user_input, chunks, chunk_sources, vectorizer, chunk_vectors)
            response = generate_response(user_input, results)

            # Add assistant response to chat
            st.session_state.messages.append({"role": "assistant", "content": response})

            # Force a rerun to update the chat display
            st.experimental_rerun()

    # Add information about the data
    with st.expander("About this chatbot"):
        st.write("This chatbot uses information extracted from your medical textbooks to answer questions. It works by finding the most relevant passages that match your query.")
        st.write("Note: This is a simple retrieval system and not a true AI. It can only provide information that exists in your textbooks.")

if __name__ == "__main__":
    main()